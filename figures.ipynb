{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "import models\n",
    "import yaml\n",
    "from trainer import Trainer\n",
    "import pprint\n",
    "from torchvision import transforms\n",
    "import datasets\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Load YAML config\n",
    "with open(\"./configs/noisycifar10/train_noisycifar10_ce_symm40.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# # read YAML file from string\n",
    "# config = yaml.safe_load(\n",
    "# \"\"\"\n",
    "# data:\n",
    "#   dataset: noisy_cifar10\n",
    "#   noise_rate: 0.4\n",
    "#   noise_type: symmetric\n",
    "#   random_seed: 42\n",
    "\n",
    "# model:\n",
    "#   architecture: resnet18\n",
    "#   num_classes: 10\n",
    "\n",
    "# wandb:\n",
    "#   mode: disabled # \"disabled\" or \"online\"\n",
    "#   entity: siit-iitp\n",
    "#   project: noisy-label\n",
    "\n",
    "# trainer:\n",
    "#   optimizer: sgd\n",
    "#   init_lr: 1.0e-1\n",
    "#   momentum: 0.9\n",
    "#   weight_decay: 1.0e-4\n",
    "#   lr_scheduler: multistep\n",
    "#   max_epoch: 200\n",
    "#   loss_fn: cross_entropy\n",
    "#   num_classes: 10\n",
    "#   num_workers: 2\n",
    "#   batch_size: 128\n",
    "#   save_model: true\n",
    "# \"\"\"\n",
    "# )\n",
    "\n",
    "model = models.get_model(**config[\"model\"]).cuda()\n",
    "model = torch.jit.script(model)\n",
    "\n",
    "\n",
    "WANDB_RUN_ID = \"mquy2drg\" # NoisyCIFAR10(symm,0.4)-CE\n",
    "# WANDB_RUN_ID = \"ccnf390c\" # NoisyCIFAR10(symm,0.4)-MAE\n",
    "# WANDB_RUN_ID = \"i0qx1u8n\" # NoisyCIFAR10(symm,0.4)-CE-AutoAugment\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "                model=model,\n",
    "                config=config['trainer'],\n",
    "                )\n",
    "\n",
    "\n",
    "def load_checkpoint(name=\"model_199.pth\"):\n",
    "    checkpoint = wandb.restore(name, run_path=f\"siit-iitp/noisy-label/{WANDB_RUN_ID}\", replace=True, root='./temp')\n",
    "    trainer.model.load_state_dict(torch.load(checkpoint.name, map_location=\"cuda\"))\n",
    "    print(f\"Loaded checkpoint: {checkpoint.name}\")\n",
    "\n",
    "print(trainer.criterion)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "auto_augment = transforms_v2.AutoAugment(AutoAugmentPolicy.CIFAR10)\n",
    "random_crop = transforms.Compose([\n",
    "    transforms_v2.RandomCrop(32, padding=4),\n",
    "    transforms_v2.RandomHorizontalFlip(),\n",
    "])\n",
    "num_augment = 100\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.tensor(np.array(x)).permute(2,0,1)),\n",
    "    transforms.Lambda(lambda x: torch.stack([auto_augment(x) for _ in range(num_augment)] + [random_crop(x) for _ in range(num_augment)], dim=0)), # (11, C, H, W)\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset, _ = datasets.get_dataset(**config[\"data\"])\n",
    "train_dataset.transform = transform_train\n",
    "\n",
    "config[\"trainer\"][\"num_workers\"] = 32\n",
    "config[\"trainer\"][\"batch_size\"] = 200\n",
    "dataloader = trainer.get_dataloader(train_dataset, train=False)\n",
    "\n",
    "sampled_dataset = {\n",
    "    'image': [],\n",
    "    'target': [],\n",
    "    'target_gt': [],\n",
    "}\n",
    "for batch in tqdm.tqdm(dataloader):\n",
    "    sampled_dataset['image'].append(batch['image'])\n",
    "    sampled_dataset['target'].append(batch['target'])\n",
    "    sampled_dataset['target_gt'].append(batch['target_gt'])\n",
    "sampled_dataset = {k: torch.cat(v, dim=0) for k, v in sampled_dataset.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch.utils.data\n",
    "train_dataset = torch.utils.data.TensorDataset(*list(sampled_dataset.values()))\n",
    "dataloader = trainer.get_dataloader(train_dataset, train=False)\n",
    "\n",
    "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010), inplace=True)\n",
    "dp_model = torch.nn.DataParallel(trainer.model)\n",
    "\n",
    "def run_inference(i):\n",
    "    load_checkpoint(name=f\"model_{i}.pth\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        trainer.model.eval()\n",
    "        result = {\n",
    "            'output': [],\n",
    "            'target': [],\n",
    "            'target_gt': [],\n",
    "        }\n",
    "        for batch in tqdm.tqdm(dataloader):\n",
    "            data = batch[0].cuda().float().div_(255.0)\n",
    "            target = batch[1].cuda()\n",
    "            target_gt = batch[2].cuda() # (B,)\n",
    "\n",
    "            b = data.size(0)\n",
    "            data = normalize(data)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = dp_model(data.view(-1,3,32,32)) # (B*11, 10)\n",
    "            result['output'].append(output.view(b, -1, 10))\n",
    "            result['target'].append(target)\n",
    "            result['target_gt'].append(target_gt)\n",
    "\n",
    "        result = {k: torch.cat(v, dim=0).cpu() for k, v in result.items()}\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\n",
    "        'autoaugment': torch.unbind(result['output'][:,:num_augment,:]),\n",
    "        'randomcrop': torch.unbind(result['output'][:,num_augment:,:]),\n",
    "        'is_noisy': (result['target'] != result['target_gt']).tolist(), # (50000, 11, 10)\n",
    "        'target': result['target'].tolist(), # (50000,)\n",
    "        'target_gt': result['target_gt'].tolist(), # (50000,)\n",
    "        })\n",
    "    torch.save(df, f'./model_{i}_result.pt')\n",
    "\n",
    "\n",
    "for k in range(9, 200, 10):\n",
    "    run_inference(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n",
      "Files already downloaded and verified\n",
      "Loaded checkpoint: ./temp/model_199.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31aeb6664004a989cef09c99f5c83ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "import models\n",
    "import yaml\n",
    "from trainer import Trainer\n",
    "import pprint\n",
    "from torchvision import transforms\n",
    "import datasets\n",
    "import torchvision.transforms.v2 as transforms_v2\n",
    "from torchvision.transforms import AutoAugmentPolicy\n",
    "import tqdm.auto as tqdm\n",
    "import einops\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Load YAML config\n",
    "with open(\"./configs/noisycifar10/train_noisycifar10_ce_symm40.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# # read YAML file from string\n",
    "# config = yaml.safe_load(\n",
    "# \"\"\"\n",
    "# data:\n",
    "#   dataset: noisy_cifar10\n",
    "#   noise_rate: 0.4\n",
    "#   noise_type: symmetric\n",
    "#   random_seed: 42\n",
    "\n",
    "# model:\n",
    "#   architecture: resnet18\n",
    "#   num_classes: 10\n",
    "\n",
    "# wandb:\n",
    "#   mode: disabled # \"disabled\" or \"online\"\n",
    "#   entity: siit-iitp\n",
    "#   project: noisy-label\n",
    "\n",
    "# trainer:\n",
    "#   optimizer: sgd\n",
    "#   init_lr: 1.0e-1\n",
    "#   momentum: 0.9\n",
    "#   weight_decay: 1.0e-4\n",
    "#   lr_scheduler: multistep\n",
    "#   max_epoch: 200\n",
    "#   loss_fn: cross_entropy\n",
    "#   num_classes: 10\n",
    "#   num_workers: 2\n",
    "#   batch_size: 128\n",
    "#   save_model: true\n",
    "# \"\"\"\n",
    "# )\n",
    "\n",
    "model = models.get_model(**config[\"model\"])\n",
    "model.fc = torch.nn.Identity()\n",
    "model = model.cuda()\n",
    "model = torch.jit.script(model)\n",
    "\n",
    "\n",
    "WANDB_RUN_ID = \"mquy2drg\" # NoisyCIFAR10(symm,0.4)-CE\n",
    "# WANDB_RUN_ID = \"ccnf390c\" # NoisyCIFAR10(symm,0.4)-MAE\n",
    "# WANDB_RUN_ID = \"i0qx1u8n\" # NoisyCIFAR10(symm,0.4)-CE-AutoAugment\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "                model=model,\n",
    "                config=config['trainer'],\n",
    "                )\n",
    "\n",
    "\n",
    "def load_checkpoint(name=\"model_199.pth\"):\n",
    "    checkpoint = wandb.restore(name, run_path=f\"siit-iitp/noisy-label/{WANDB_RUN_ID}\", replace=True, root='./temp')\n",
    "    trainer.model.load_state_dict(torch.load(checkpoint.name, map_location=\"cuda\"), strict=False)\n",
    "    print(f\"Loaded checkpoint: {checkpoint.name}\")\n",
    "\n",
    "print(trainer.criterion)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "auto_augment = transforms_v2.AutoAugment(AutoAugmentPolicy.CIFAR10)\n",
    "random_crop = transforms.Compose([\n",
    "    transforms_v2.RandomCrop(32, padding=4),\n",
    "    transforms_v2.RandomHorizontalFlip(),\n",
    "])\n",
    "num_augment = 100\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.tensor(np.array(x)).permute(2,0,1)),\n",
    "    transforms.Lambda(lambda x: torch.stack([auto_augment(x) for _ in range(num_augment)] + [random_crop(x) for _ in range(num_augment)], dim=0)), # (11, C, H, W)\n",
    "    transforms.Lambda(lambda x: x.float().div_(255.0)),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010), inplace=True),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset, _ = datasets.get_dataset(**config[\"data\"])\n",
    "train_dataset.transform = transform_train\n",
    "\n",
    "config[\"trainer\"][\"num_workers\"] = 8\n",
    "config[\"trainer\"][\"batch_size\"] = 200\n",
    "dataloader = trainer.get_dataloader(train_dataset, train=False)\n",
    "\n",
    "\n",
    "dp_model = torch.nn.DataParallel(trainer.model)\n",
    "\n",
    "load_checkpoint(name=f\"model_199.pth\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    trainer.model.eval()\n",
    "    result = {\n",
    "        'output': [],\n",
    "        'target': [],\n",
    "        'target_gt': [],\n",
    "    }\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        data = batch['image'].cuda()\n",
    "        target = batch['target']\n",
    "        target_gt = batch['target_gt'] # (B,)\n",
    "\n",
    "        b = data.size(0)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = dp_model(einops.rearrange(data, 'b n c h w -> (b n) c h w')) # (B*11, 10)\n",
    "        result['output'].append(einops.rearrange(output, '(b n) c -> b n c', n=2*num_augment).cpu().float())\n",
    "        result['target'].append(target)\n",
    "        result['target_gt'].append(target_gt)\n",
    "\n",
    "    result = {k: torch.cat(v, dim=0).cpu() for k, v in result.items()}\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame({\n",
    "#     'autoaugment': torch.unbind(result['output'][:,:num_augment,:]),\n",
    "#     'randomcrop': torch.unbind(result['output'][:,num_augment:,:]),\n",
    "#     'is_noisy': (result['target'] != result['target_gt']).tolist(), # (50000, 11, 10)\n",
    "#     'target': result['target'].tolist(), # (50000,)\n",
    "#     'target_gt': result['target_gt'].tolist(), # (50000,)\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(result, f'./embddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "result = torch.load('./embddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_augment=100\n",
    "temp = {\n",
    "    'autoaugment': result['output'][:,:num_augment,:],\n",
    "    'randomcrop': result['output'][:,num_augment:,:],\n",
    "    'is_noisy': result['target'] != result['target_gt'], # (50000, 11, 10)\n",
    "    'target': result['target'], # (50000,)\n",
    "    'target_gt': result['target_gt'], # (50000,)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(temp, f'./embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.load('./embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'autoaugment': torch.unbind(result['output'][:,:num_augment,:]),\n",
    "    'randomcrop': torch.unbind(result['output'][:,num_augment:,:]),\n",
    "    'is_noisy': (result['target'] != result['target_gt']).tolist(), # (50000, 11, 10)\n",
    "    'target': result['target'].tolist(), # (50000,)\n",
    "    'target_gt': result['target_gt'].tolist(), # (50000,)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10240, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import numpy.random as nprn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "df = torch.load('./figure3.pt')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[df['is_noisy']==False]\n",
    "df_noisy = df[df['is_noisy']==True]\n",
    "\n",
    "\n",
    "\n",
    "def get_projection_matrix(k):\n",
    "    theta = (2*np.pi/k) * (np.arange(k) + 0.5)\n",
    "    return np.stack([np.sin(theta), np.cos(theta)], axis=0)\n",
    "\n",
    "num_classes = 10 # CIFAR-10\n",
    "T = get_projection_matrix(num_classes)\n",
    "\n",
    "\n",
    "cifar10_classes = ['airplane$\\mathcal{T}$', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "fig = plt.figure(figsize=(35,20))\n",
    "for idx in range(28):\n",
    "    row = df_noisy.iloc[idx]\n",
    "    # row = df_clean.iloc[idx]\n",
    "    blue = row['autoaugment_logits'].float().div(1).softmax(-1).numpy()\n",
    "    # pointpred = row['pointpred'][None, :].softmax(-1).numpy()\n",
    "    red = row['randomcrop_logits'].float().div(1).softmax(-1).numpy()\n",
    "\n",
    "    blue = T @ blue.T\n",
    "    red = T @ red.T\n",
    "\n",
    "    ax = fig.add_subplot(4,7,idx + 1)\n",
    "\n",
    "    # Plot the octagon\n",
    "    vertices = T @ np.eye(num_classes)\n",
    "    vertices = np.hstack([vertices, vertices[:, 0:1]])\n",
    "    for i, vertex in enumerate(vertices[:,:-1].T):\n",
    "        ax.text(vertex[0], vertex[1], cifar10_classes[i], ha='center', va='center', fontsize=12)\n",
    "        ax.plot([0, vertex[0]], [0, vertex[1]], 'k--', linewidth=1)\n",
    "\n",
    "    # Plot the points\n",
    "    ax.scatter(blue[0], blue[1], linewidth=0, alpha=0.3, zorder=10)\n",
    "    ax.scatter([red[0]], [red[1]], color='red', alpha = 0.3, s=50, linewidth=0, zorder=12)\n",
    "\n",
    "    # Set the axis limits\n",
    "    ax.set_xlim(-1.2, 1.2)\n",
    "    ax.set_ylim(-1.2, 1.2)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f\"target: {cifar10_classes[row['target']]}, gt: {cifar10_classes[row['target_gt']]}\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure3.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ece(conf, correct, bin_width=0.1):\n",
    "\n",
    "    bins = []\n",
    "    bin_width = 0.1\n",
    "    ece = 0.0\n",
    "    for low in np.arange(0.0, 1.0, bin_width):\n",
    "        mask = (confidence >= low) & (confidence < low + bin_width)\n",
    "        acc = correct[mask].float().mean().item()\n",
    "        bins.append(acc)\n",
    "        if mask.sum() > 0:\n",
    "            ece += np.abs(acc - (low + bin_width/2)) * mask.sum()\n",
    "    bins = np.nan_to_num(bins)\n",
    "    ece = ece / len(conf)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    # ax.bar([f\"{x:.2f}\" for x in np.arange(0.0, 1.0, bin_width)], bins)\n",
    "    ax.bar(np.arange(0.0, 1.0, bin_width), bins, align='edge', width=0.1)\n",
    "    print(bins)\n",
    "    ax.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "    ax.set_title(f\"{ece=:.2f}\")\n",
    "\n",
    "    # set the xlim to (0,1)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure6.pdf')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# correct = torch.stack(tuple(df['logits']), dim=0).float().softmax(-1).argmax(-1) == torch.tensor(df['target_gt'])\n",
    "# confidence = torch.stack(tuple(df['logits']), dim=0).float().softmax(-1).max(-1).values\n",
    "# plot_ece(confidence, correct)\n",
    "\n",
    "# correct = torch.stack(tuple(df['randomcrop_logits']), dim=0).float().softmax(-1).mean(1).argmax(-1) == torch.tensor(df['target_gt'])\n",
    "# confidence = torch.stack(tuple(df['randomcrop_logits']), dim=0).float().softmax(-1).mean(1).max(-1).values\n",
    "# plot_ece(confidence, correct)\n",
    "\n",
    "correct = torch.stack(tuple(df['autoaugment_logits']), dim=0).float().softmax(-1).mean(1).argmax(-1) == torch.tensor(df['target_gt'])\n",
    "confidence = torch.stack(tuple(df['autoaugment_logits']), dim=0).float().softmax(-1).mean(1).max(-1).values\n",
    "plot_ece(confidence, correct)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5\n",
    "\n",
    "- https://wandb.ai/siit-iitp/noisy-label_baselines/runs/auc5c96t\n",
    "- https://wandb.ai/siit-iitp/noisy-label_baselines/runs/kapwliv4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7-(a)\n",
    "\n",
    "- https://wandb.ai/siit-iitp/noisy-label/runs/i0qx1u8n\n",
    "- https://wandb.ai/siit-iitp/noisy-label/runs/niuft3hk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7-(b)\n",
    "\n",
    "- https://wandb.ai/siit-iitp/noisy-label/runs/uv2cklac\n",
    "- https://wandb.ai/siit-iitp/noisy-label/runs/2bd0sdnd\n",
    "- https://wandb.ai/siit-iitp/noisy-label/runs/niuft3hk\n",
    "- https://wandb.ai/siit-iitp/noisy-label/runs/mquy2drg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 8\n",
    "\n",
    "- https://wandb.ai/siit-iitp/noisy-label/runs/h4hlrxf9\n",
    "- https://wandb.ai/siit-iitp/noisy-label/runs/k1cilssr\n",
    "- https://wandb.ai/siit-iitp/noisy-label/runs/uv2cklac\n",
    "- https://wandb.ai/siit-iitp/noisy-label/runs/2bd0sdnd\n",
    "- https://wandb.ai/siit-iitp/noisy-label/runs/mquy2drg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
